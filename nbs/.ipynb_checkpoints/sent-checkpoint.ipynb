{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Brand Sentiment Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tweet_text, emotion_in_tweet_is_directed_at, is_there_an_emotion_directed_at_a_brand_or_product \n",
    "train_file = 'judge-1377884607_tweet_product_company.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode_unicode(data):\n",
    "    unicode_error_counter = 0\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    for tweet, label in data:\n",
    "        try:\n",
    "            tweets.append(tweet.decode('utf8'))\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            unicode_error_counter += 1\n",
    "    return tweets, labels, unicode_error_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(fn):\n",
    "    df = pd.read_csv(fn)\n",
    "    tweets = df['tweet_text']\n",
    "    labels = df['is_there_an_emotion_directed_at_a_brand_or_product']\n",
    "    \n",
    "    tweets = tweets[pd.notnull(tweets)]\n",
    "    labels = labels[pd.notnull(labels)]\n",
    "\n",
    "    data = zip(tweets.get_values(), labels.get_values())\n",
    "    tweets, labels, errors = decode_unicode(data)\n",
    "            \n",
    "    return tweets, labels, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows 8613 8613 unicode errors 479\n",
      "[u'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'] \n",
      "\n",
      "['Negative emotion'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text, target, errors = read_data(path+train_file)\n",
    "print 'rows', len(text), len(target), 'unicode errors', errors\n",
    "print text[0:1], '\\n'\n",
    "print target[0:1], '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "  (0, 4467)\t1\n",
      "  (0, 5052)\t1\n",
      "  (0, 5563)\t1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "count_vect.fit(text)\n",
    "\n",
    "print count_vect.vocabulary_.get(u'3g')\n",
    "print count_vect.transform([\"I love my iphone!!!\"])\n",
    "#print counts[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "counts = count_vect.transform(text)\n",
    "\n",
    "nb.fit(counts[0:6000], target[0:6000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive emotion']\n"
     ]
    }
   ],
   "source": [
    "print nb.predict(count_vect.transform([\"I love my iphone!!!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.551090700344\n",
      "[ 0.52375435  0.51448436  0.51216686  0.51911935  0.53077816  0.54302326\n",
      "  0.56046512  0.54651163  0.52209302  0.51744186]\n",
      "0.528983796174\n"
     ]
    }
   ],
   "source": [
    "predictions = nb.predict(counts)\n",
    "print 'Accuracy on training data:',sum(predictions[6000:8613] == target[6000:8613]) / len(target[6000:8613])\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "scores = cross_validation.cross_val_score(nb, counts, target, cv=10)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.59212051  0.59212051  0.59212051  0.59212051  0.59233449  0.59302326\n",
      "  0.59302326  0.59302326  0.59302326  0.59302326]\n",
      "0.592593281324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "scores = cross_validation.cross_val_score(dc, counts, target, cv=10)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[(u'counts', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "       ...t 0x7f9f3605ede8>)), (u'multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "p = Pipeline(steps=[('counts', CountVectorizer(ngram_range=(1, 2))),\n",
    "                ('feature_selection', SelectKBest(chi2, k=10000)),\n",
    "                ('multinomialnb', MultinomialNB())])\n",
    "\n",
    "p.fit(text, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5561993   0.53997683  0.53070684  0.50984936  0.54936121  0.51860465\n",
      "  0.57093023  0.55232558  0.55348837  0.52325581]\n",
      "0.540469818815\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validation.cross_val_score(p, text, target, cv=10)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization (GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'counts__max_df': (0.5, 0.75, 1.0),\n",
    "    'counts__min_df': (1, 2, 3),\n",
    "    'counts__ngram_range': ((1,1), (1,2)),\n",
    "    'feature_selection__k': (100, 200, 300, 'all')\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(p, parameters, n_jobs=1, verbose=1, cv=10)\n",
    "\n",
    "grid_search.fit(text, target)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tweet_text, emotion_in_tweet_is_directed_at, is_there_an_emotion_directed_at_a_brand_or_product \n",
    "test_file = 'crowdflower-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_text, test_target, _ = read_data(path+test_file)\n",
    "\n",
    "# print test_target[0:1], '\\n'\n",
    "# print test_text[0:1], '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
